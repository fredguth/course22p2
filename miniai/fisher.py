# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09c_FisherCallback.ipynb.

# %% ../nbs/09c_FisherCallback.ipynb 5
from __future__ import annotations
from wwf.utils import state_versions
from nbdev import show_doc
import random,math,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
import fastcore.all as fc
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from torch import tensor,nn,optim
import torch.nn.functional as F
from torch.distributions import Categorical
import torchvision.transforms.functional as TF
from datasets import load_dataset
import datasets
import time
from .datasets import *
from .conv import *
from .learner import *
from .activations import *
from .init import *
from torcheval.metrics import MulticlassAccuracy,Mean
from tqdm.auto import tqdm

# %% auto 0
__all__ = ['lin', 'get_simple_model', 'FisherCB', 'FisherInfo', 'ProgressMetricsCB', 'Natural']

# %% ../nbs/09c_FisherCallback.ipynb 8
def lin(ni, nf, act=nn.ReLU, norm=None, bias=None):
    if bias is None: bias = not isinstance(norm, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d))
    layers = [nn.Linear(ni, nf, bias=bias)]
    if norm: layers.append(norm(nf))
    if act: layers.append(act())
    return nn.Sequential(*layers)

# %% ../nbs/09c_FisherCallback.ipynb 11
def get_simple_model(act=nn.ReLU, nhs=None, norm=None):
    m = 28*28
    if nhs is None: nhs = [m//(2**i) for i in range(6)] # [784, 392, 196, 98, 49, 24]
    layers = [lin(nhs[i], nhs[i+1], act, norm) for i in range(len(nhs)-1)]
    return nn.Sequential(nn.Flatten(),*layers, lin(nhs[-1],10, act=None, norm=False, bias=True)).to(def_device)

# %% ../nbs/09c_FisherCallback.ipynb 28
class FisherCB(TrainCB):
    def before_fit(self):
        self.learn.fisher = self
        self.Fs = []
        self.Iy = []
        self.FIM = torch.zeros_like(torch.cat([p.data.unsqueeze(0).flatten() for p in self.learn.model.parameters() if p.requires_grad]))
        self.i = 0
    def after_batch(self):
        if self.learn.model.training: self.Iy.append([v.compute() for k,v in self.learn.metrics.all_metrics.items() if k=='accuracy'][0])
    def backward(self):
        self.learn.loss.backward()
        if self.learn.model.training:
            F = torch.cat([(p.grad**2).unsqueeze(0).flatten() for p in self.learn.model.parameters() if (hasattr(p,'grad'))])
            self.FIM += F
            self.Fs.append(to_cpu(F))
            self.i += 1

# %% ../nbs/09c_FisherCallback.ipynb 31
from torcheval.metrics import Metric
class FisherInfo(Metric[torch.Tensor]):
    def __init__(self, device = None, fisher_cb= None):
        super().__init__(device=device)
        self.fcb = fisher_cb
        self.fisher_info = 0
    def update(self, input: torch.Tensor, target: torch.Tensor): return self
    def compute(self):
        self.fisher_info = (self.fcb.FIM.sum()/self.fcb.i) if (self.fcb.FIM is not None and self.fcb.i>0) else 0
        return self.fisher_info
    def merge_state(self): return self
    @property
    def value(self):
        return self.fisher_info
    @property
    def name(self): return 'fisher'

# %% ../nbs/09c_FisherCallback.ipynb 35
from fastprogress import progress_bar,master_bar
class ProgressMetricsCB(Callback):
    order = MetricsCB.order+1
    def __init__(self, plot=False): self.plot = plot
    def before_fit(self):
        self.learn.epochs = self.mbar = master_bar(self.learn.epochs)
        self.mbar.names=[k for k,v in self.learn.metrics.all_metrics.items()]
        if hasattr(self.learn, 'metrics'): self.learn.metrics._log = self._log
        self.metrics=[]
    def _log(self, d): self.mbar.write(str(d))
    def before_epoch(self): self.learn.dl = progress_bar(self.learn.dl, leave=False, parent=self.mbar)
    def after_batch(self):
        if self.plot and hasattr(self.learn, 'metrics') and self.training:
            self.metrics.append([v.compute().detach().cpu() for k,v in self.learn.metrics.all_metrics.items()])
            self.mbar.update_graph([[fc.L.range(m),m] for m in zip(*self.metrics)])

# %% ../nbs/09c_FisherCallback.ipynb 41
class Natural(SGD):
    def __init__(self, params, lr, wd=0., eps=5e-5):
        super().__init__(params, lr=lr, wd=wd)
        self.eps = eps
        self.i = 0
    def opt_step(self, p):
        self.i+=1
        if not hasattr(p, 'grad2'): p.grad2 = torch.zeros_like(p.grad.data)
        p.grad2 += p.grad ** 2
        p -= self.lr * (p.grad / ((p.grad2/self.i) + self.eps))

set_seed(42)
